{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3079e370",
   "metadata": {},
   "source": [
    "# Gu√≠a Completa de T√©cnicas de Prompting\n",
    "\n",
    "## 1. T√©cnicas B√°sicas\n",
    "\n",
    "### 1.1 Claridad y Especificidad\n",
    "**Principio**: Ser claro y espec√≠fico en lugar de vago.\n",
    "\n",
    "**‚ùå Prompt vago:**\n",
    "```\n",
    "Escribe sobre marketing\n",
    "```\n",
    "\n",
    "**‚úÖ Prompt espec√≠fico:**\n",
    "```\n",
    "Escribe un art√≠culo de 800 palabras sobre estrategias de marketing digital para peque√±as empresas de e-commerce, enfoc√°ndose en redes sociales, SEO y email marketing.\n",
    "```\n",
    "\n",
    "### 1.2 Contexto Detallado\n",
    "**Principio**: Proporcionar contexto relevante para obtener respuestas m√°s precisas.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Act√∫a como un consultor de recursos humanos con 10 a√±os de experiencia. Una startup tecnol√≥gica de 25 empleados necesita implementar un sistema de evaluaci√≥n de desempe√±o. Proporciona un plan detallado considerando el presupuesto limitado y la cultura informal de la empresa.\n",
    "```\n",
    "\n",
    "## 2. T√©cnicas de Estructura\n",
    "\n",
    "### 2.1 Chain of Thought (Cadena de Pensamiento)\n",
    "**Principio**: Pedir al AI que muestre su razonamiento paso a paso.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Resuelve este problema paso a paso:\n",
    "Una tienda vende camisetas a $15 cada una. Si compras 3 o m√°s, hay un 20% de descuento. Si compras 5 o m√°s, hay un 30% de descuento. ¬øCu√°nto costar√≠a comprar 7 camisetas?\n",
    "\n",
    "Piensa paso a paso:\n",
    "1. Identifica qu√© descuento aplica\n",
    "2. Calcula el precio con descuento\n",
    "3. Calcula el total\n",
    "```\n",
    "\n",
    "### 2.2 Few-Shot Learning (Aprendizaje con Pocos Ejemplos)\n",
    "**Principio**: Proporcionar ejemplos para establecer el patr√≥n deseado.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Convierte estas descripciones a formato de producto e-commerce:\n",
    "\n",
    "Ejemplo 1:\n",
    "Input: \"Zapatos deportivos Nike rojos talla 42\"\n",
    "Output: \"Zapatillas Nike Running - Rojo | Talla 42 | Ideales para correr y entrenar\"\n",
    "\n",
    "Ejemplo 2:\n",
    "Input: \"Laptop HP 8GB RAM 256GB SSD\"\n",
    "Output: \"Laptop HP Pavilion - 8GB RAM, 256GB SSD | Perfecta para trabajo y estudio\"\n",
    "\n",
    "Ahora convierte:\n",
    "Input: \"Auriculares inal√°mbricos Sony cancelaci√≥n ruido\"\n",
    "```\n",
    "\n",
    "### 2.3 Role Playing (Interpretaci√≥n de Rol)\n",
    "**Principio**: Asignar un rol espec√≠fico al AI para obtener respuestas m√°s especializadas.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Act√∫a como un chef profesional con especializaci√≥n en cocina mediterr√°nea. Un cliente quiere un men√∫ para una cena rom√°ntica de 3 platos usando ingredientes de temporada de primavera. El presupuesto es moderado y uno de los comensales es vegetariano.\n",
    "```\n",
    "\n",
    "## 3. T√©cnicas Avanzadas\n",
    "\n",
    "### 3.1 Tree of Thoughts (√Årbol de Pensamientos)\n",
    "**Principio**: Explorar m√∫ltiples l√≠neas de razonamiento antes de llegar a una conclusi√≥n.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Necesito elegir una plataforma para mi tienda online. Analiza estas opciones considerando m√∫ltiples perspectivas:\n",
    "\n",
    "Opciones: Shopify, WooCommerce, Magento\n",
    "\n",
    "Eval√∫a desde estas perspectivas:\n",
    "1. Perspectiva t√©cnica (facilidad de uso, personalizaci√≥n)\n",
    "2. Perspectiva financiera (costos iniciales y recurrentes)\n",
    "3. Perspectiva de crecimiento (escalabilidad)\n",
    "4. Perspectiva de marketing (herramientas integradas)\n",
    "\n",
    "Para cada perspectiva, analiza los pros y contras de cada opci√≥n, luego sintetiza una recomendaci√≥n final.\n",
    "```\n",
    "\n",
    "### 3.2 Constitutional AI\n",
    "**Principio**: Establecer principios o reglas que deben seguirse en la respuesta.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Crea un plan de marketing para una aplicaci√≥n de fitness siguiendo estos principios:\n",
    "1. Debe ser √©tico y no usar t√°cticas manipuladoras\n",
    "2. Debe ser inclusivo para todos los tipos de cuerpo\n",
    "3. Debe enfocarse en bienestar general, no solo en apariencia\n",
    "4. Debe ser honesto sobre los resultados esperados\n",
    "5. Debe promover h√°bitos sostenibles a largo plazo\n",
    "```\n",
    "\n",
    "### 3.3 Prompt Chaining (Encadenamiento)\n",
    "**Principio**: Dividir tareas complejas en pasos secuenciales.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Paso 1: Analiza las tendencias actuales del mercado de aplicaciones m√≥viles de salud mental\n",
    "Paso 2: Bas√°ndote en el an√°lisis anterior, identifica 3 nichos espec√≠ficos con potencial\n",
    "Paso 3: Para el nicho m√°s prometedor, desarrolla un concepto de aplicaci√≥n\n",
    "Paso 4: Crea un plan de lanzamiento para esa aplicaci√≥n\n",
    "```\n",
    "\n",
    "## 4. T√©cnicas de Control de Salida\n",
    "\n",
    "### 4.1 Formato Espec√≠fico\n",
    "**Principio**: Especificar exactamente c√≥mo debe estructurarse la respuesta.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Analiza la estrategia de contenido de una marca de ropa sostenible y presenta tu an√°lisis en este formato:\n",
    "\n",
    "## An√°lisis de Estrategia de Contenido\n",
    "\n",
    "**Marca:** [Nombre]\n",
    "**Audiencia objetivo:** [Descripci√≥n]\n",
    "\n",
    "### Fortalezas\n",
    "- [Lista de 3-4 puntos]\n",
    "\n",
    "### Debilidades\n",
    "- [Lista de 3-4 puntos]\n",
    "\n",
    "### Recomendaciones\n",
    "1. **Corto plazo (1-3 meses):** [Acciones espec√≠ficas]\n",
    "2. **Mediano plazo (3-6 meses):** [Acciones espec√≠ficas]\n",
    "3. **Largo plazo (6+ meses):** [Acciones espec√≠ficas]\n",
    "\n",
    "### M√©tricas de √©xito\n",
    "- [3-4 KPIs espec√≠ficos]\n",
    "```\n",
    "\n",
    "### 4.2 Restricciones de Longitud y Tono\n",
    "**Ejemplo:**\n",
    "```\n",
    "Explica el concepto de inteligencia artificial en exactamente 100 palabras, usando un tono conversacional como si le hablaras a un adolescente de 15 a√±os. Evita jerga t√©cnica y usa analog√≠as simples.\n",
    "```\n",
    "\n",
    "### 4.3 Formato JSON/Estructurado\n",
    "**Ejemplo:**\n",
    "```\n",
    "Analiza este producto y devuelve el resultado en formato JSON:\n",
    "\n",
    "{\n",
    "  \"producto\": \"[nombre del producto]\",\n",
    "  \"categoria\": \"[categor√≠a principal]\",\n",
    "  \"puntos_fuertes\": [\"[lista de fortalezas]\"],\n",
    "  \"puntos_debiles\": [\"[lista de debilidades]\"],\n",
    "  \"precio_sugerido\": \"[rango de precio]\",\n",
    "  \"mercado_objetivo\": \"[descripci√≥n del p√∫blico]\",\n",
    "  \"puntuacion_viabilidad\": \"[1-10]\"\n",
    "}\n",
    "```\n",
    "\n",
    "## 5. T√©cnicas de Optimizaci√≥n\n",
    "\n",
    "### 5.1 Prompts Negativos\n",
    "**Principio**: Especificar qu√© NO hacer para evitar resultados no deseados.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Escribe un art√≠culo sobre inversi√≥n para principiantes.\n",
    "\n",
    "NO incluyas:\n",
    "- Consejos espec√≠ficos de acciones o criptomonedas\n",
    "- Promesas de ganancias garantizadas\n",
    "- Jerga financiera compleja sin explicar\n",
    "- Recomendaciones de productos financieros espec√≠ficos\n",
    "\n",
    "S√ç incluye:\n",
    "- Conceptos b√°sicos explicados claramente\n",
    "- Principios generales de inversi√≥n\n",
    "- Advertencias sobre riesgos\n",
    "- Pasos pr√°cticos para empezar\n",
    "```\n",
    "\n",
    "### 5.2 Iteraci√≥n y Refinamiento\n",
    "**Ejemplo de proceso iterativo:**\n",
    "```\n",
    "Primera iteraci√≥n:\n",
    "\"Crea un plan de contenido para redes sociales\"\n",
    "\n",
    "Segunda iteraci√≥n:\n",
    "\"El plan anterior es muy gen√©rico. Cr√©alo espec√≠ficamente para una panader√≠a artesanal local, enfoc√°ndose en Instagram y Facebook, con 3 publicaciones semanales durante 1 mes\"\n",
    "\n",
    "Tercera iteraci√≥n:\n",
    "\"Perfecto. Ahora incluye las mejores horas para publicar y hashtags espec√≠ficos para cada publicaci√≥n\"\n",
    "```\n",
    "\n",
    "### 5.3 Meta-Prompting\n",
    "**Principio**: Pedir al AI que ayude a mejorar el prompt.\n",
    "\n",
    "**Ejemplo:**\n",
    "```\n",
    "Quiero crear un prompt para generar ideas de contenido para mi blog de viajes. Mi prompt actual es: \"Dame ideas para mi blog de viajes\"\n",
    "\n",
    "¬øC√≥mo puedo mejorar este prompt para obtener ideas m√°s espec√≠ficas y √∫tiles? Sugiere una versi√≥n mejorada y explica por qu√© ser√≠a m√°s efectiva.\n",
    "```\n",
    "\n",
    "## 6. T√©cnicas para Casos Espec√≠ficos\n",
    "\n",
    "### 6.1 An√°lisis y S√≠ntesis\n",
    "```\n",
    "Act√∫a como un analista de mercado. Analiza la siguiente informaci√≥n sobre la industria de food delivery y sintetiza:\n",
    "\n",
    "[Datos/informaci√≥n]\n",
    "\n",
    "Estructura tu an√°lisis en:\n",
    "1. Tendencias principales (3-4 puntos clave)\n",
    "2. Oportunidades identificadas\n",
    "3. Riesgos y desaf√≠os\n",
    "4. Recomendaci√≥n estrat√©gica (1 p√°rrafo)\n",
    "```\n",
    "\n",
    "### 6.2 Creatividad Guiada\n",
    "```\n",
    "Genera 5 conceptos creativos para una campa√±a publicitaria de una marca de caf√© org√°nico. Para cada concepto incluye:\n",
    "\n",
    "- T√≠tulo de la campa√±a\n",
    "- Concepto central en 1 l√≠nea\n",
    "- Canal principal (digital, tradicional, experiencial)\n",
    "- P√∫blico objetivo espec√≠fico\n",
    "- Elemento diferenciador\n",
    "\n",
    "Criterios: Debe ser aut√©ntico, sostenible y memorable.\n",
    "```\n",
    "\n",
    "### 6.3 Resoluci√≥n de Problemas\n",
    "```\n",
    "Problema: Una peque√±a empresa de software tiene alta rotaci√≥n de empleados (40% anual).\n",
    "\n",
    "Usando el m√©todo de los \"5 Por Qu√©s\", analiza las posibles causas ra√≠z y luego propone 3 soluciones espec√≠ficas y accionables, prioriz√°ndolas por impacto y facilidad de implementaci√≥n.\n",
    "```\n",
    "\n",
    "## 7. Mejores Pr√°cticas\n",
    "\n",
    "### 7.1 Checklist de un Buen Prompt\n",
    "- [ ] Contexto claro establecido\n",
    "- [ ] Objetivo espec√≠fico definido\n",
    "- [ ] Formato de salida especificado\n",
    "- [ ] Ejemplos incluidos (si es necesario)\n",
    "- [ ] Restricciones y limitaciones mencionadas\n",
    "- [ ] Criterios de √©xito establecidos\n",
    "\n",
    "### 7.2 Errores Comunes a Evitar\n",
    "1. **Ser demasiado vago:** \"Ay√∫dame con marketing\"\n",
    "2. **No dar contexto:** Asumir que el AI conoce tu situaci√≥n espec√≠fica\n",
    "3. **Pedir demasiado en un solo prompt:** Intentar resolver m√∫ltiples problemas complejos\n",
    "4. **No especificar formato:** Dejar que el AI decida c√≥mo estructurar la respuesta\n",
    "5. **No iterar:** Conformarse con la primera respuesta sin refinar\n",
    "\n",
    "### 7.3 Consejos para Prompts M√°s Efectivos\n",
    "- **Usa verbos de acci√≥n espec√≠ficos:** \"Analiza\", \"Compara\", \"Dise√±a\", \"Eval√∫a\"\n",
    "- **Incluye m√©tricas cuando sea relevante:** \"Aumentar engagement en 25%\"\n",
    "- **Especifica el nivel de detalle:** \"Explicaci√≥n b√°sica\", \"An√°lisis profundo\"\n",
    "- **Menciona el p√∫blico objetivo:** \"Para directivos\", \"Para principiantes\"\n",
    "- **Establece deadline o urgencia si es relevante:** \"Para implementar la pr√≥xima semana\"\n",
    "\n",
    "## 8. Plantillas √ötiles\n",
    "\n",
    "### Plantilla para An√°lisis de Competencia\n",
    "```\n",
    "Analiza a [COMPETIDOR] en la industria de [INDUSTRIA] considerando:\n",
    "\n",
    "**Contexto:** [Tu empresa/situaci√≥n]\n",
    "**Objetivo:** [Qu√© buscas lograr con este an√°lisis]\n",
    "\n",
    "**√Åreas a analizar:**\n",
    "1. Propuesta de valor\n",
    "2. Estrategia de pricing\n",
    "3. Canales de distribuci√≥n\n",
    "4. Estrategia de marketing\n",
    "5. Fortalezas y debilidades\n",
    "\n",
    "**Formato de entrega:** [Especificar formato deseado]\n",
    "**Pr√≥ximos pasos:** Incluir 3 recomendaciones accionables\n",
    "```\n",
    "\n",
    "### Plantilla para Brainstorming\n",
    "```\n",
    "Necesito generar ideas para [PROBLEMA/DESAF√çO].\n",
    "\n",
    "**Contexto:**\n",
    "- Industria: [industria]\n",
    "- Audiencia: [descripci√≥n]\n",
    "- Presupuesto: [rango]\n",
    "- Restricciones: [limitaciones]\n",
    "\n",
    "**Tipo de ideas buscadas:** [espec√≠fico, creativo, pr√°ctico, etc.]\n",
    "**Cantidad:** [n√∫mero de ideas]\n",
    "**Formato:** Para cada idea incluir [t√≠tulo, descripci√≥n, pros/contras, nivel de dificultad]\n",
    "```\n",
    "\n",
    "### Plantilla para Resoluci√≥n de Problemas\n",
    "```\n",
    "**Problema:** [Descripci√≥n clara del problema]\n",
    "**Impacto:** [C√≥mo afecta al negocio/proyecto]\n",
    "**Contexto:** [Informaci√≥n relevante]\n",
    "**Recursos disponibles:** [Tiempo, presupuesto, equipo]\n",
    "**Restricciones:** [Limitaciones]\n",
    "\n",
    "**Proceso solicitado:**\n",
    "1. An√°lisis de causas ra√≠z\n",
    "2. 3-5 opciones de soluci√≥n\n",
    "3. Evaluaci√≥n de cada opci√≥n (pros/contras/recursos)\n",
    "4. Recomendaci√≥n final con plan de implementaci√≥n\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215e160",
   "metadata": {},
   "source": [
    "## üß† What Is RAG and Why Should You Care?\n",
    "\n",
    "**RAG (Retrieval Augmented Generation)** is one of the most powerful techniques in modern AI applications. Let's break it down:\n",
    "\n",
    "| Component | What It Does | Why It Matters |\n",
    "|-----------|--------------|----------------|\n",
    "| **Retrieval** | Finds relevant information from your documents | Ensures answers come from *your* data, not just the AI's training |\n",
    "| **Augmentation** | Enhances the AI's knowledge with this specific information | Makes responses accurate and up-to-date |\n",
    "| **Generation** | Creates human-like responses using the retrieved information | Delivers insights in natural, easy-to-understand language |\n",
    "\n",
    "<div style=\"background-color: #effaf5; border: 1px solid #0d9488; padding: 15px; margin: 20px 0; border-radius: 5px;\">\n",
    "<h4 style=\"color: #000000; margin-top: 0;\">üí° Real-World Analogy</h4>\n",
    "<p style=\"color: #000000;\">Think of RAG as the difference between:</p>\n",
    "<ul style=\"color: #000000;\">\n",
    "<li><strong>A general knowledge expert</strong> who studied years ago (standard LLM)</li>\n",
    "<li><strong>A specialist with your documents open</strong> in front of them, referencing exact paragraphs as they answer your questions (RAG system)</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "## üõ†Ô∏è Our Exciting Toolkit\n",
    "\n",
    "We'll be using several cutting-edge tools to build our RAG system:\n",
    "\n",
    "| Tool | What It Is | Why It's Amazing |\n",
    "|------|------------|------------------|\n",
    "| **Ollama** | An open-source platform that runs AI models locally on your computer | Privacy (your data never leaves your machine), no API costs, and complete control |\n",
    "| **ChromaDB** | A specialized database for storing and searching \"vector embeddings\" | Lightning-fast semantic search that understands meaning, not just keywords |\n",
    "| **LangChain** | A framework that connects AI components together like building blocks | Makes complex AI workflows simple and customizable |\n",
    "| **Gradio** | A tool for creating web interfaces for AI models | Turns your code into a professional-looking application in minutes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain_ollama gradio chromadb pypdf langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd694c20",
   "metadata": {},
   "source": [
    "# üéØ What We'll Build Together\n",
    "\n",
    "By the end of this tutorial, you'll have created:\n",
    "\n",
    "```\n",
    "üìÑ Documents ‚Üí üî™ Chunker ‚Üí üßÆ Vector DB ‚Üí üîç Retriever ‚Üí ü§ñ LLM ‚Üí üí¨ Answer\n",
    "```\n",
    "\n",
    "A complete RAG system that can:\n",
    "\n",
    "1. **Process PDF documents** of your choice\n",
    "2. **Break them into smart chunks** that preserve meaning\n",
    "3. **Transform text into vectors** that capture semantic meaning\n",
    "4. **Store everything efficiently** for lightning-fast retrieval\n",
    "5. **Find the most relevant information** for any question\n",
    "6. **Generate accurate, helpful responses** with proper citations\n",
    "\n",
    "<div style=\"background-color: #ffe4e6; border-left: 6px solid #be123c; padding: 15px; margin: 20px 0; border-radius: 5px;\">\n",
    "<h3 style=\"color: #000000; margin-top: 0;\">üî• Why This Matters For Your Career</h3>\n",
    "<p style=\"color: #000000;\">RAG systems are at the forefront of practical AI applications. At MAIA Academy, we've seen how companies are rapidly adopting this technology to:</p>\n",
    "<ul style=\"color: #000000;\">\n",
    "<li>Build intelligent document assistants</li>\n",
    "<li>Create knowledge bases that actually answer questions</li>\n",
    "<li>Develop customer support systems that handle complex queries</li>\n",
    "<li>Implement research tools that synthesize information from multiple sources</li>\n",
    "</ul>\n",
    "<p style=\"color: #000000;\">The skills you'll learn today are directly transferable to real-world AI projects and align perfectly with our <strong>Foundations of AI Development</strong> and <strong>Deep Learning & LLMs</strong> modules!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbeeda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\demst\\Desktop\\rag\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# Gradio for web interface\n",
    "import gradio as gr\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b9af63",
   "metadata": {},
   "source": [
    "## 2. Setting the Stage: Configuration\n",
    "\n",
    "<div style=\"background-color: #2d333b; padding: 20px; border-radius: 8px; margin-bottom: 20px; border-left: 6px solid #58a6ff;\">\n",
    "  <h3 style=\"color: #ffffff; margin-top: 0;\">System Configuration Parameters</h3>\n",
    "  <p style=\"color: #ffffff;\">Before we build our RAG system, we need to configure some important settings‚Äîlike tuning a new instrument before a performance. These parameters will determine how our system processes and interacts with documents.</p>\n",
    "</div>\n",
    "\n",
    "<table style=\"width: 100%; border-collapse: collapse; margin: 20px 0; background-color: #22272e;\">\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; width: 200px;\"><strong style=\"color: #58a6ff;\">PERSIST_DIRECTORY</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">Where we'll store our \"data safe\" (the vector database) on disk. This allows our system to remember what it learned even after restarting.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">CHUNK_SIZE</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">How big each text piece will be (in characters). This affects how much context the AI has when answering questions.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">CHUNK_OVERLAP</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">How much the pieces overlap to maintain context between chunks and ensure no information is lost at the boundaries.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">PDF_URLS</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The documents we'll use as our knowledge base (our \"reference library\"). These are the sources the system will learn from.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">LLM_MODEL</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The \"brain\" that processes the context and generates answers (like llama3 or other models available in Ollama).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56;\"><strong style=\"color: #58a6ff;\">EMBEDDING_MODEL</strong></td>\n",
    "    <td style=\"padding: 15px; border: 1px solid #444c56; color: #adbac7;\">The \"translator\" that converts text into numerical vectors that capture meaning. Different models balance between speed and accuracy.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border: 1px solid #444c56; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h3 style=\"color: #58a6ff; margin-top: 0;\">üí° What's This Chunk Stuff?</h3>\n",
    "  <p style=\"color: #adbac7;\">Think of cutting a big sandwich. If the pieces are huge, you get more filling but it's hard to bite. If they're tiny, you bite easy but might miss the full flavor. Overlap is like leaving a bit of the last bite on the next one so you don't lose track of the overall taste.</p>\n",
    "  \n",
    "  <div style=\"display: flex; justify-content: space-between; margin-top: 20px; text-align: center;\">\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Large Chunks (2000+)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ More context<br>‚úÖ Better for complex topics<br>‚ùå Less precise retrieval<br>‚ùå Slower processing</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Medium Chunks (800-1200)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ Balanced approach<br>‚úÖ Good for most cases<br>‚úÖ Reasonable speed<br>‚úÖ Decent precision</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin: 0 10px;\">\n",
    "      <p style=\"color: #58a6ff;\"><strong>Small Chunks (300-500)</strong></p>\n",
    "      <p style=\"color: #adbac7;\">‚úÖ Very precise retrieval<br>‚úÖ Fast processing<br>‚ùå Limited context<br>‚ùå May miss broader concepts</p>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; background-color: #22272e; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #ff7b72; font-weight: bold;\">‚ö†Ô∏è Warning</p>\n",
    "  <p style=\"color: #adbac7;\">This notebook is designed to be read with a dark background. If you program with a white background, just know that you're a complete psychopath and a danger to society.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33657fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "PERSIST_DIRECTORY = \"chroma_db\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "PDF_URLS = [ \n",
    "    \"https://www.ine.es/daco/daco42/ecp/ecp0123.pdf\",\n",
    "    \"https://fundacionalternativas.org/wp-content/uploads/2023/10/PERSONAS_MIGRANTES_v02.pdf\"\n",
    "]\n",
    "LLM_MODEL = \"llama3.2:1b\"  # Using a small Llama model for faster responses\n",
    "EMBEDDING_MODEL = \"all-minilm\"  # Small, fast embedding model (22M parameters)\n",
    "TEMPERATURE = 0.1  # Lower temperature for more deterministic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07093b22",
   "metadata": {},
   "source": [
    "## 3. The Heart of It: RAGSystem Class\n",
    "\n",
    "Now, let‚Äôs create the main ‚Äúrobot‚Äù that does all the work: the RAGSystem class. This robot gets ready with all the tools it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67b0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, pdf_urls: List[str], persist_directory: str = PERSIST_DIRECTORY):\n",
    "        self.pdf_urls = pdf_urls\n",
    "        self.persist_directory = persist_directory\n",
    "        self.documents = []\n",
    "        self.vectorstore = None\n",
    "        self.llm = None\n",
    "        self.chain = None\n",
    "        \n",
    "        # Initialize the LLM with streaming capability\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = ChatOllama(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            callback_manager=callback_manager\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "        \n",
    "        logger.info(f\"Initialized RAG system with {len(pdf_urls)} PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3102d2",
   "metadata": {},
   "source": [
    "## 3. Building Our RAG System\n",
    "\n",
    "<div style=\"background-color: #2d333b; padding: 5px; border-radius: 4px; margin-bottom: 10px;\">\n",
    "  <h3 style=\"color: #58a6ff; margin: 10px;\">3.1 Loading and Chopping Documents</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 15px; border-radius: 8px; border-left: 4px solid #7ee787; margin-bottom: 20px;\">\n",
    "  <p style=\"color: #adbac7;\">The first crucial step in our RAG pipeline is to read PDFs and slice them into manageable chunks. This process transforms raw documents into pieces our system can effectively process.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border: 1px solid #444c56; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h4 style=\"color: #7ee787; margin-top: 0;\">üìö Why Do We Chop?</h4>\n",
    "  \n",
    "  <p style=\"color: #adbac7;\">Imagine a huge cake: you can't eat it all at once, so you cut it into slices. Same with documents:</p>\n",
    "  \n",
    "  <ul style=\"color: #adbac7; margin-left: 20px;\">\n",
    "    <li><strong style=\"color: #d2a8ff;\">AI has a \"small tummy\"</strong> (a context window that limits how much text it can process at once)</li>\n",
    "    <li><strong style=\"color: #d2a8ff;\">Small chunks help find exact answers fast</strong> (better retrieval precision)</li>\n",
    "    <li><strong style=\"color: #d2a8ff;\">It makes the system quicker and less likely to choke</strong> (more efficient processing)</li>\n",
    "  </ul>\n",
    "  \n",
    "  <div style=\"margin-top: 25px; background-color: #22272e; padding: 15px; border-radius: 5px; border: 1px dashed #444c56;\">\n",
    "    <h5 style=\"color: #58a6ff; margin-top: 0;\">üîç Technical Insight: The Chunking Process</h5>\n",
    "    <p style=\"color: #adbac7;\">Our system uses a <code style=\"background-color: #2d333b; padding: 2px 5px; border-radius: 3px; color: #ff7b72;\">RecursiveCharacterTextSplitter</code> that intelligently divides text based on:</p>\n",
    "    <ul style=\"color: #adbac7;\">\n",
    "      <li>Natural boundaries (paragraphs, sentences)</li>\n",
    "      <li>Configured chunk size (how many characters per chunk)</li>\n",
    "      <li>Strategic overlap to maintain context between chunks</li>\n",
    "    </ul>\n",
    "    <p style=\"color: #adbac7;\">This ensures that each chunk contains coherent, meaningful information rather than arbitrary text divisions.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; background-color: #22272e; border-radius: 8px; overflow: hidden; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; padding: 15px; border-right: 1px solid #444c56;\">\n",
    "    <p style=\"color: #58a6ff; font-weight: bold; margin-top: 0;\">Document Loading</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Reading PDFs using PyPDFLoader</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Extracting text and metadata</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Handling multiple documents</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px; border-right: 1px solid #444c56;\">\n",
    "    <p style=\"color: #58a6ff; font-weight: bold; margin-top: 0;\">Document Chunking</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Splitting into smaller pieces</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Maintaining logical boundaries</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Creating overlapping sections</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px;\">\n",
    "    <p style=\"color: #58a6ff; font-weight: bold; margin-top: 0;\">Result</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Dozens or hundreds of chunks</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Each ~1000 characters long</p>\n",
    "    <p style=\"color: #adbac7; margin-bottom: 0;\">üëâ Ready for embedding creation</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border-left: 4px solid #f97583; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #adbac7; margin: 0;\"><strong style=\"color: #f97583;\">‚ö†Ô∏è Common Pitfall:</strong> Setting your chunk size too small (under 300 characters) or too large (over 2000 characters) can severely impact your system's performance. Start with ~1000 and adjust based on your specific documents and query needs.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9248f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(self) -> None:\n",
    "    \"\"\"Load and split PDF documents\"\"\"\n",
    "    logger.info(\"Loading and processing PDFs...\")\n",
    "    \n",
    "    # Text splitter for chunking documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    all_pages = []\n",
    "    for url in self.pdf_urls:\n",
    "        try:\n",
    "            loader = PyPDFLoader(url)\n",
    "            pages = loader.load()\n",
    "            logger.info(f\"Loaded {len(pages)} pages from {url}\")\n",
    "            all_pages.extend(pages)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF from {url}: {e}\")\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    self.documents = text_splitter.split_documents(all_pages)\n",
    "    logger.info(f\"Created {len(self.documents)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c68a46",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2d333b; padding: 5px; border-radius: 4px; margin-bottom: 10px;\">\n",
    "  <h3 style=\"color: #58a6ff; margin: 10px;\">3.2 Storing in a Vector Database</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 15px; border-radius: 8px; border-left: 4px solid #f0883e; margin-bottom: 20px;\">\n",
    "  <p style=\"color: #adbac7;\">After chunking our documents, we need to store them in a way that allows for intelligent searching. This is where vectors and ChromaDB come into play.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border: 1px solid #444c56; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h4 style=\"color: #f0883e; margin-top: 0;\">üßÆ What Are Vectors?</h4>\n",
    "  \n",
    "  <p style=\"color: #adbac7;\">Think of each chunk as a person, and we give it a unique \"fingerprint\" based on what it says. These fingerprints are actually lists of numbers that capture meaning.</p>\n",
    "  \n",
    "  <div style=\"display: flex; margin-top: 20px; background-color: #22272e; padding: 15px; border-radius: 8px;\">\n",
    "    <div style=\"flex: 1; padding-right: 15px;\">\n",
    "      <p style=\"color: #adbac7; font-style: italic; margin-top: 0;\">\"I like the sun\"</p>\n",
    "      <p style=\"color: #d2a8ff; font-family: monospace; font-size: 0.9em;\">[0.12, -0.33, 0.65, ...]</p>\n",
    "    </div>\n",
    "    <div style=\"flex: 1; padding-left: 15px; border-left: 1px dashed #444c56;\">\n",
    "      <p style=\"color: #adbac7; font-style: italic; margin-top: 0;\">\"I love the heat\"</p>\n",
    "      <p style=\"color: #d2a8ff; font-family: monospace; font-size: 0.9em;\">[0.15, -0.28, 0.61, ...]</p>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <p style=\"color: #adbac7; margin-top: 20px;\">These sentences get similar vector \"fingerprints\" because they express similar concepts. This lets us search by <strong>meaning</strong>, not just exact words.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; background-color: #22272e; border-radius: 8px; overflow: hidden; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center; border-right: 1px solid #444c56;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">1</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Convert</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">Text ‚Üí Vector</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center; border-right: 1px solid #444c56;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">2</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Store</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">In ChromaDB</p>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 15px; display: flex; flex-direction: column; align-items: center; text-align: center;\">\n",
    "    <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;\">\n",
    "      <span style=\"color: #58a6ff; font-weight: bold; font-size: 1.5em;\">3</span>\n",
    "    </div>\n",
    "    <p style=\"color: #f0883e; font-weight: bold; margin: 5px 0;\">Retrieve</p>\n",
    "    <p style=\"color: #adbac7; margin: 5px 0;\">By Similarity</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 20px; border-radius: 8px; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <h4 style=\"color: #58a6ff; margin-top: 0;\">In Plain English:</h4>\n",
    "  \n",
    "  <p style=\"color: #adbac7;\">1. <strong>We transform text into numbers</strong> using the embedding model (all-minilm)</p>\n",
    "  <p style=\"color: #adbac7;\">2. <strong>We store these numbers in ChromaDB</strong> along with the original text</p>\n",
    "  <p style=\"color: #adbac7;\">3. <strong>When you ask a question</strong>, we convert your question to a vector too</p>\n",
    "  <p style=\"color: #adbac7;\">4. <strong>ChromaDB finds chunks with similar vectors</strong> to your question</p>\n",
    "  <p style=\"color: #adbac7;\">5. <strong>These similar chunks</strong> likely contain the answer you need</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; background-color: #22272e; border-radius: 8px; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; padding: 20px;\">\n",
    "    <h5 style=\"color: #7ee787; margin-top: 0;\">üí° Why This Is Cool</h5>\n",
    "    <ul style=\"color: #adbac7; list-style-type: none; padding-left: 0;\">\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Finds similar concepts</strong>, even with different words</li>\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Lightning-fast search</strong> of large document collections</li>\n",
    "      <li style=\"margin-bottom: 8px;\">‚úÖ <strong>Works across languages</strong> (Spanish \"sol\" ‚âà English \"sun\")</li>\n",
    "      <li>‚úÖ <strong>More accurate</strong> than keyword searching</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border-left: 4px solid #d2a8ff; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #adbac7; margin: 0;\"><strong style=\"color: #d2a8ff;\">üöÄ Pro Tip:</strong> Think of it like searching a music library - you find songs that \"sound similar\" to the one you like, not just songs with the exact same title.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6c7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(self) -> None:\n",
    "    \"\"\"Create a fresh vector database\"\"\"\n",
    "    # Remove any existing database\n",
    "    if os.path.exists(self.persist_directory):\n",
    "        import shutil\n",
    "        logger.info(f\"Removing existing vectorstore at {self.persist_directory}\")\n",
    "        shutil.rmtree(self.persist_directory, ignore_errors=True)\n",
    "    \n",
    "    # Create a new vectorstore\n",
    "    logger.info(\"Creating new vectorstore...\")\n",
    "    if not self.documents:\n",
    "        self.load_documents()\n",
    "    \n",
    "    # Create a temporary directory for the database\n",
    "    # This helps avoid permission issues on some systems\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    logger.info(f\"Using temporary directory for initial database creation: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # First create in temp directory\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=temp_dir\n",
    "        )\n",
    "        \n",
    "        # Now create the real directory\n",
    "        if not os.path.exists(self.persist_directory):\n",
    "            os.makedirs(self.persist_directory)\n",
    "            \n",
    "        # And create the final vectorstore\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.persist_directory\n",
    "        )\n",
    "        self.vectorstore.persist()\n",
    "        \n",
    "        logger.info(f\"Vectorstore created successfully with {len(self.documents)} documents\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vectorstore: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up temp directory\n",
    "        if os.path.exists(temp_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095c38d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #2d333b; padding: 5px; border-radius: 4px; margin-bottom: 10px;\">\n",
    "  <h3 style=\"color: #58a6ff; margin: 10px;\">3.3 Building the RAG Chain</h3>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #22272e; padding: 15px; border-radius: 8px; border-left: 4px solid #79c0ff; margin-bottom: 20px;\">\n",
    "  <p style=\"color: #adbac7;\">Here's where our system turns into a \"detective.\" We connect all the components into a sequence that transforms questions into accurate answers.</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #1c2128; padding: 20px; border-radius: 8px; margin: 20px 0; border: 1px solid #444c56;\">\n",
    "  <h4 style=\"color: #79c0ff; margin-top: 0; text-align: center; margin-bottom: 20px;\">The RAG Chain Components</h4>\n",
    "  \n",
    "  <!-- Retriever Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üîç</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Searcher (Retriever)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Turns your question into a fingerprint and finds the closest matches in the database.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Prompt Template Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üìù</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Instructions (Prompt)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Like a recipe: \"Be nice, use the chunks, cite your sources.\" This keeps answers helpful and trustworthy.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">prompt = PromptTemplate.from_template(template)</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- LLM Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 15px; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">üß†</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The AI (LLM)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Writes the final response based on the instructions and retrieved chunks.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">llm = ChatOllama(model=\"llama3\", temperature=0.1)</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Output Parser Component -->\n",
    "  <div style=\"background-color: #22272e; border-radius: 8px; padding: 15px; margin-bottom: 0; border: 1px solid #444c56;\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <div style=\"background-color: #2d333b; width: 50px; height: 50px; border-radius: 50%; display: flex; align-items: center; justify-content: center; margin-right: 15px;\">\n",
    "        <span style=\"font-size: 1.5em;\">‚ú®</span>\n",
    "      </div>\n",
    "      <div>\n",
    "        <p style=\"color: #79c0ff; margin: 0; font-weight: bold;\">The Formatter (Parser)</p>\n",
    "      </div>\n",
    "    </div>\n",
    "    <div style=\"margin-top: 10px; padding-left: 65px;\">\n",
    "      <p style=\"color: #adbac7; margin: 0;\">Makes the response neat and clear for the user to read.</p>\n",
    "      <div style=\"background-color: #2d333b; padding: 8px; border-radius: 4px; margin-top: 10px;\">\n",
    "        <code style=\"color: #d2a8ff; font-size: 0.9em;\">StrOutputParser()</code>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<!-- Flow diagram -->\n",
    "<div style=\"background-color: #22272e; padding: 20px; border-radius: 8px; margin: 20px 0;\">\n",
    "  <h4 style=\"color: #79c0ff; margin-top: 0; text-align: center;\">How It All Flows Together</h4>\n",
    "  \n",
    "  <div style=\"display: flex; justify-content: center; align-items: center; flex-wrap: wrap; margin: 20px 0;\">\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">‚ùì</div>\n",
    "      <div style=\"color: #adbac7;\">Question</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üîç</div>\n",
    "      <div style=\"color: #adbac7;\">Retriever</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üìù</div>\n",
    "      <div style=\"color: #adbac7;\">Prompt</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üß†</div>\n",
    "      <div style=\"color: #adbac7;\">LLM</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">‚ú®</div>\n",
    "      <div style=\"color: #adbac7;\">Parser</div>\n",
    "    </div>\n",
    "    <div style=\"font-size: 1.5em; margin: 0 10px; color: #adbac7;\">‚Üí</div>\n",
    "    <div style=\"text-align: center; background-color: #2d333b; padding: 15px; border-radius: 8px; margin: 5px;\">\n",
    "      <div style=\"font-size: 2em; margin-bottom: 5px;\">üí°</div>\n",
    "      <div style=\"color: #adbac7;\">Answer</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"background-color: #1c2128; padding: 15px; border-radius: 8px; margin-top: 20px;\">\n",
    "    <p style=\"color: #adbac7; margin: 0; text-align: center;\">This entire chain is created with just a few lines of code:</p>\n",
    "    <div style=\"background-color: #2d333b; border-radius: 5px; padding: 15px; margin-top: 10px; font-family: monospace;\">\n",
    "      <pre style=\"color: #d2a8ff; margin: 0; overflow-x: auto; font-size: 0.9em;\">self.chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | self.llm\n",
    "    | StrOutputParser()\n",
    ")</pre>\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #2d333b; border-left: 4px solid #58a6ff; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "  <p style=\"color: #adbac7; margin: 0;\"><strong style=\"color: #adbac7;\">üí° Pro Tip:</strong> The key to a good RAG system is balance. A great prompt template with poor retrieval won't work well, and perfect retrieval with bad instructions will still give bad answers. All pieces need to work together!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a758ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chain(self) -> None:\n",
    "    \"\"\"Set up the RAG chain for question answering\"\"\"\n",
    "    if not self.vectorstore:\n",
    "        self.create_vectorstore()\n",
    "    \n",
    "    # Create retriever with search parameters\n",
    "    retriever = self.vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5}  # Return top 5 most relevant chunks\n",
    "    )\n",
    "    \n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "    ### INSTRUCTIONS: \n",
    "    You are an AI assistant dedicated to answering questions in a polite and professional manner. You must provide a helpful response to the user.\n",
    "    \n",
    "    (1) Be attentive to details: read the question and context thoroughly before answering.\n",
    "    (2) Begin your response with a friendly tone and reiterate the question to ensure you understood it.\n",
    "    (3) If the context allows you to answer the question, write a detailed, helpful, and easy-to-understand response, with sources referenced in the text. IF NOT: if you cannot find the answer, respond with an explanation, starting with: \"I couldn't find the information in the documents I have access to.\"\n",
    "    (4) Below your response, please list all referenced sources (i.e., document sections that support your claims).\n",
    "    (5) Review your answer to ensure you answered the question, the response is helpful and professional, and it's formatted to be easily readable.\n",
    "    \n",
    "    THINK STEP BY STEP\n",
    "    \n",
    "    Answer the following question using the provided context.\n",
    "    ### Question: {question} ###\n",
    "    ### Context: {context} ###     \n",
    "    ### Helpful Answer with Sources:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    # Create the chain\n",
    "    self.chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | self.llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    logger.info(\"RAG chain setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af5a77",
   "metadata": {},
   "source": [
    "### 3.4 Answering Questions\n",
    "\n",
    "Time to shine! The robot takes your question, processes it, and gives you an answer. If something goes wrong, it politely lets you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a65baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(self, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a question using the RAG chain\n",
    "    \n",
    "    Args:\n",
    "        question: The question to answer\n",
    "        \n",
    "    Returns:\n",
    "        The answer to the question\n",
    "    \"\"\"\n",
    "    if not self.chain:\n",
    "        self.setup_chain()\n",
    "    \n",
    "    logger.info(f\"Answering question: {question}\")\n",
    "    try:\n",
    "        answer = self.chain.invoke(question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error answering question: {e}\")\n",
    "        return f\"Error processing your question: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472cba3",
   "metadata": {},
   "source": [
    "## 4. A Window to the World: Gradio Interface\n",
    "\n",
    "Let‚Äôs make our robot user-friendly with a web interface.\n",
    "\n",
    "### üåê Why Gradio?\n",
    "It‚Äôs like building an app with Lego blocks: easy, fast, and you can use it from your phone or computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c011a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gradio_interface(rag_system: RAGSystem) -> gr.Interface:\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for the RAG system\n",
    "    \n",
    "    Args:\n",
    "        rag_system: The RAG system to use\n",
    "        \n",
    "    Returns:\n",
    "        A Gradio interface\n",
    "    \"\"\"\n",
    "    def get_answer(question: str) -> str:\n",
    "        \"\"\"Wrapper function for the Gradio interface\"\"\"\n",
    "        return rag_system.answer_question(question)\n",
    "    \n",
    "    # Gradio interface configuration\n",
    "    interface = gr.Interface(\n",
    "        fn=get_answer,\n",
    "        inputs=gr.Textbox(\n",
    "            placeholder=\"Ask a question about immigration...\",\n",
    "            label=\"Your Question\"\n",
    "        ),\n",
    "        outputs=gr.Markdown(label=\"Answer\"),\n",
    "        title=\"Document Intelligence System with LLM\",\n",
    "        description=\"Ask any question about immigration based on the loaded documents\",\n",
    "        theme=gr.themes.Soft(),\n",
    "        allow_flagging=\"never\",\n",
    "        examples=[\n",
    "            \"How many immigrants arrive each year?\",\n",
    "            \"What are the main countries of origin?\",\n",
    "            \"What economic impact does immigration have?\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a5acb",
   "metadata": {},
   "source": [
    "## 5. Let‚Äôs Get It Running!\n",
    "\n",
    "The ‚Äústart button‚Äù checks everything, tests a question, and opens the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8319d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Main function to run the RAG system\"\"\"\n",
    "    try:\n",
    "        # Display available models\n",
    "        print(\"\\n==== CHECKING OLLAMA MODELS ====\")\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            print(\"Available Ollama models:\")\n",
    "            if response.status_code == 200:\n",
    "                for model in response.json().get(\"models\", []):\n",
    "                    print(f\"- {model['name']}\")\n",
    "            else:\n",
    "                print(f\"Error checking Ollama models: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to Ollama: {e}\")\n",
    "        \n",
    "        print(f\"\\nUsing LLM model: {LLM_MODEL}\")\n",
    "        print(f\"Using embedding model: {EMBEDDING_MODEL}\")\n",
    "        print(\"Make sure these models are available with 'ollama pull' commands.\")\n",
    "        \n",
    "        # Create and initialize the RAG system\n",
    "        rag_system = RAGSystem(pdf_urls=PDF_URLS)\n",
    "        \n",
    "        # Load documents and create vectorstore\n",
    "        rag_system.load_documents()\n",
    "        rag_system.create_vectorstore()\n",
    "        \n",
    "        # Test with a control question\n",
    "        logger.info(\"Testing with a control question...\")\n",
    "        test_answer = rag_system.answer_question(\"How many immigrants arrive each year?\")\n",
    "        logger.info(f\"Control answer received (length: {len(test_answer)})\")\n",
    "        \n",
    "        # Create and launch Gradio interface\n",
    "        logger.info(\"Launching Gradio interface...\")\n",
    "        interface = create_gradio_interface(rag_system)\n",
    "        interface.launch(share=False)  # Set share=True to create a public link\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in the main function: {e}\")\n",
    "        print(f\"\\n\\nERROR: {str(e)}\\n\\n\")\n",
    "        print(\"\\nTROUBLESHOOTING TIPS:\")\n",
    "        print(\"1. Make sure Ollama is running: 'ollama serve'\")\n",
    "        print(f\"2. Make sure you have pulled the required models:\")\n",
    "        print(f\"   - ollama pull {LLM_MODEL}\")\n",
    "        print(f\"   - ollama pull {EMBEDDING_MODEL}\")\n",
    "        print(\"3. If you're still having dimension issues, try using a different embedding model by changing EMBEDDING_MODEL\")\n",
    "        print(\"4. Check that you have the required Python packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3001fcfd",
   "metadata": {},
   "source": [
    "## 6. Putting It Together\n",
    "\n",
    "Time to assemble our robot and make it run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "735f07f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAGSystem.load_documents = load_documents\n",
    "RAGSystem.create_vectorstore = create_vectorstore\n",
    "RAGSystem.setup_chain = setup_chain\n",
    "RAGSystem.answer_question = answer_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== CHECKING OLLAMA MODELS ====\n",
      "Available Ollama models:\n",
      "- all-minilm:latest\n",
      "- llama3.2:1b\n",
      "- codeqwen:7b\n",
      "- terminator:latest\n",
      "- qwen3:0.6b\n",
      "- qwen3:1.7b\n",
      "\n",
      "Using LLM model: llama3.2:1b\n",
      "Using embedding model: all-minilm\n",
      "Make sure these models are available with 'ollama pull' commands.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\demst\\AppData\\Local\\Temp\\ipykernel_61380\\1044174799.py:23: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
      "  rag_system = RAGSystem(pdf_urls=PDF_URLS)\n",
      "2025-05-24 01:26:56,649 - __main__ - INFO - Initialized RAG system with 2 PDFs\n",
      "2025-05-24 01:26:56,651 - __main__ - INFO - Loading and processing PDFs...\n",
      "2025-05-24 01:26:59,419 - __main__ - INFO - Loaded 5 pages from https://www.ine.es/daco/daco42/ecp/ecp0123.pdf\n",
      "2025-05-24 01:27:03,324 - __main__ - INFO - Loaded 37 pages from https://fundacionalternativas.org/wp-content/uploads/2023/10/PERSONAS_MIGRANTES_v02.pdf\n",
      "2025-05-24 01:27:03,331 - __main__ - INFO - Created 165 document chunks\n",
      "2025-05-24 01:27:03,333 - __main__ - INFO - Creating new vectorstore...\n",
      "2025-05-24 01:27:03,334 - __main__ - INFO - Using temporary directory for initial database creation: C:\\Users\\demst\\AppData\\Local\\Temp\\tmpkcgs9sqf\n",
      "2025-05-24 01:27:05,178 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-05-24 01:27:17,659 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-24 01:27:17,932 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-05-24 01:27:24,774 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\demst\\AppData\\Local\\Temp\\ipykernel_61380\\1117243408.py:37: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  self.vectorstore.persist()\n",
      "2025-05-24 01:27:24,996 - __main__ - INFO - Vectorstore created successfully with 165 documents\n",
      "2025-05-24 01:27:24,999 - __main__ - INFO - Testing with a control question...\n",
      "2025-05-24 01:27:25,011 - __main__ - INFO - RAG chain setup complete\n",
      "2025-05-24 01:27:25,012 - __main__ - INFO - Answering question: How many immigrants arrive each year?\n",
      "2025-05-24 01:27:25,078 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-24 01:27:38,986 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I understand that you are seeking information on the number of immigrants who arrive each year in Spain.\n",
      "\n",
      "According to the provided document, the estimated number of migrants arriving in Spain is around 180,000 per year (Figura 5: N√∫mero de adquisiciones de nacionalidad espa√±ola de personas residentes, 2013-2022). This data is based on the information presented in Figure 6: Poblaci√≥n extranjera por comunidades aut√≥nomas, 2022.\n",
      "\n",
      "The document also mentions that the number of migrants arriving in Spain has been steadily increasing over the years, with a significant increase from 2013 to 2022. The exact breakdown of migrant arrivals by year is not provided, but it is mentioned that there were 225,793 migrants in 2013 and 205,880 in 2022.\n",
      "\n",
      "It's worth noting that the document also provides information on the number of migrants arriving in different regions of Spain, with some areas having significantly higher numbers than others. For example, the region of Galicia has an estimated population of over 41,756 people (Figura 6), while the region of Extremadura has an estimated population of around 1 million people.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* Figura 5: N√∫mero de adquisiciones de nacionalidad espa√±ola de personas residentes, 2013-2022. (Figure 5)\n",
      "* Figura 6: Poblaci√≥n extranjera por comunidades aut√≥nomas, 2022. (Figure 6)\n",
      "\n",
      "I couldn't find the information in the documents I have access to.\n",
      "\n",
      "Please let me know if you would like me to provide further clarification or details on this topic."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 01:27:54,695 - __main__ - INFO - Control answer received (length: 1485)\n",
      "2025-05-24 01:27:54,696 - __main__ - INFO - Launching Gradio interface...\n",
      "c:\\Users\\demst\\Desktop\\rag\\rag\\Lib\\site-packages\\gradio\\interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 01:27:55,594 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-05-24 01:27:55,651 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 01:27:56,057 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-05-24 01:29:00,045 - __main__ - INFO - Answering question: cual fue la nacionalidad de inmigrantes que mas recibimos\n",
      "2025-05-24 01:29:00,115 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-24 01:29:17,700 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Hola! Me alegra poder ayudarte con tu pregunta.\n",
      "\n",
      "La nacionalidad de los inmigrantes que m√°s recibieron es un tema complejo y multifac√©tico. Seg√∫n el documento proporcionado, la participaci√≥n de los inmigrantes en la sociedad de acogida y su integraci√≥n en las instituciones p√∫blicas son fundamentales para hacer visibles sus contribuciones.\n",
      "\n",
      "De acuerdo con el texto, \"la integraci√≥n implica el respeto de los valores b√°sicos de la Uni√≥n Europea\" (CoE 1991), lo que sugiere que la nacionalidad no es un factor determinante en la integraci√≥n. Adem√°s, se menciona que \"el primer PECI incorporaba la idea de establecer un 'sistema de recepci√≥n' para personas inmigrantes reci√©n llegadas y aquellos en situaciones especialmente vulnerables\" (CoE 1991).\n",
      "\n",
      "En cuanto a las pol√≠ticas p√∫blicas, particularmente en educaci√≥n, empleo, servicios sociales, salud y vivienda, se enfatiza la necesidad de asegurar el acceso de la poblaci√≥n inmigrante a estos servicios en igualdad de condiciones con la poblaci√≥n aut√≥ctona (CoE 1991).\n",
      "\n",
      "En resumen, no se menciona expl√≠citamente que los inmigrantes sean de una determinada nacionalidad. Sin embargo, se puede inferir que su integraci√≥n y participaci√≥n en la sociedad de acogida dependen de sus capacidades para adaptarse a las normas y valores de la sociedad espa√±ola.\n",
      "\n",
      "Referencias:\n",
      "\n",
      "* CoE (1991). \"La pol√≠tica de inmigraci√≥n en la Uni√≥n Europea\". En Comisi√≥n Europea."
     ]
    }
   ],
   "source": [
    "# Run the system\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # If running in a notebook\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
